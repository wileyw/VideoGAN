{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMgXbRiTB7C23SvjzrYAA22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wileyw/VideoGAN/blob/master/diffusion_study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN0kJsF6spZm"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U einops datasets tqdm\n",
        "# python -m pip uninstall matplotlib\n",
        "!pip install matplotlib==3.1.3\n",
        "\n",
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange, reduce\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "\n",
        "def Upsample(dim, dim_out=None):\n",
        "    return nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "        nn.Conv2d(dim, default(dim_out, dim), 3, padding=1),\n",
        "    )\n",
        "\n",
        "\n",
        "def Downsample(dim, dim_out=None):\n",
        "    # No More Strided Convolutions or Pooling\n",
        "    return nn.Sequential(\n",
        "        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
        "        nn.Conv2d(dim * 4, default(dim_out, dim), 1),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Gm5q6mRqPeNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "pCAaa351T_dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightStandardizedConv2d(nn.Conv2d):\n",
        "    \"\"\"\n",
        "    https://arxiv.org/abs/1903.10520\n",
        "    weight standardization purportedly works synergistically with group normalization\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
        "\n",
        "        weight = self.weight\n",
        "        mean = reduce(weight, \"o ... -> o 1 1 1\", \"mean\")\n",
        "        var = reduce(weight, \"o ... -> o 1 1 1\", partial(torch.var, unbiased=False))\n",
        "        normalized_weight = (weight - mean) * (var + eps).rsqrt()\n",
        "\n",
        "        return F.conv2d(\n",
        "            x,\n",
        "            normalized_weight,\n",
        "            self.bias,\n",
        "            self.stride,\n",
        "            self.padding,\n",
        "            self.dilation,\n",
        "            self.groups,\n",
        "        )\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups=8):\n",
        "        super().__init__()\n",
        "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift=None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
        "\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        scale_shift = None\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
        "            scale_shift = time_emb.chunk(2, dim=1)\n",
        "\n",
        "        h = self.block1(x, scale_shift=scale_shift)\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n"
      ],
      "metadata": {
        "id": "gKNwNM4tUBZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
        "                                    nn.GroupNorm(1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)\n"
      ],
      "metadata": {
        "id": "lS4_-aUYU6Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)\n"
      ],
      "metadata": {
        "id": "P5FNFee9VRa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        init_dim=None,\n",
        "        out_dim=None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels=3,\n",
        "        self_condition=False,\n",
        "        resnet_block_groups=4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # determine dimensions\n",
        "        self.channels = channels\n",
        "        self.self_condition = self_condition\n",
        "        input_channels = channels * (2 if self_condition else 1)\n",
        "\n",
        "        init_dim = default(init_dim, dim)\n",
        "        self.init_conv = nn.Conv2d(input_channels, init_dim, 1, padding=0) # changed to 1 and 0 from 7,3\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "        # time embeddings\n",
        "        time_dim = dim * 4\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(dim),\n",
        "            nn.Linear(dim, time_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "        )\n",
        "\n",
        "        # layers\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                        Downsample(dim_in, dim_out)\n",
        "                        if not is_last\n",
        "                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
        "            is_last = ind == (len(in_out) - 1)\n",
        "\n",
        "            self.ups.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                        Upsample(dim_out, dim_in)\n",
        "                        if not is_last\n",
        "                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.out_dim = default(out_dim, channels)\n",
        "\n",
        "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
        "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
        "\n",
        "    def forward(self, x, time, x_self_cond=None):\n",
        "        if self.self_condition:\n",
        "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
        "            x = torch.cat((x_self_cond, x), dim=1)\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "        r = x.clone()\n",
        "\n",
        "        t = self.time_mlp(time)\n",
        "\n",
        "        h = []\n",
        "\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            h.append(x)\n",
        "\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "\n",
        "            x = downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = torch.cat((x, r), dim=1)\n",
        "\n",
        "        x = self.final_res_block(x, t)\n",
        "        return self.final_conv(x)\n"
      ],
      "metadata": {
        "id": "f1V6iMA3VWU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n"
      ],
      "metadata": {
        "id": "Fd3bIiQpVlkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = 300\n",
        "\n",
        "# define beta schedule\n",
        "betas = linear_beta_schedule(timesteps=timesteps)\n",
        "\n",
        "# define alphas \n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "\n",
        "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n"
      ],
      "metadata": {
        "id": "EFZ7k6PMVr7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image\n"
      ],
      "metadata": {
        "id": "HYzsz3ZkV661"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
        "\n",
        "image_size = 128\n",
        "transform = Compose([\n",
        "    Resize(image_size),\n",
        "    CenterCrop(image_size),\n",
        "    ToTensor(), # turn into Numpy array of shape HWC, divide by 255\n",
        "    Lambda(lambda t: (t * 2) - 1),\n",
        "    \n",
        "])\n",
        "\n",
        "x_start = transform(image).unsqueeze(0)\n",
        "x_start.shape\n"
      ],
      "metadata": {
        "id": "fmGoGGriWBFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "reverse_transform = Compose([\n",
        "     Lambda(lambda t: (t + 1) / 2),\n",
        "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "     Lambda(lambda t: t * 255.),\n",
        "     Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "     ToPILImage(),\n",
        "])\n"
      ],
      "metadata": {
        "id": "uj3GZUnUWC5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_transform(x_start.squeeze())"
      ],
      "metadata": {
        "id": "mCCZQsgMWKOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward diffusion (using the nice property)\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
        "    )\n",
        "\n",
        "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n"
      ],
      "metadata": {
        "id": "yVx0WbW1WOon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_noisy_image(x_start, t):\n",
        "  # add noise\n",
        "  x_noisy = q_sample(x_start, t=t)\n",
        "\n",
        "  # turn back into PIL image\n",
        "  noisy_image = reverse_transform(x_noisy.squeeze())\n",
        "\n",
        "  return noisy_image\n"
      ],
      "metadata": {
        "id": "OnuTX_cHWSRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take time step\n",
        "t = torch.tensor([40])\n",
        "\n",
        "get_noisy_image(x_start, t)\n"
      ],
      "metadata": {
        "id": "vdUZ65f_WVP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# use seed for reproducability\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0]) + with_orig\n",
        "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        row = [image] + row if with_orig else row\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if with_orig:\n",
        "        axs[0, 0].set(title='Original image')\n",
        "        axs[0, 0].title.set_size(8)\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "aZSEET9qWY__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
      ],
      "metadata": {
        "id": "p-_u0aXqWcux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
        "    predicted_noise = denoise_model(x_noisy, t)\n",
        "\n",
        "    if loss_type == 'l1':\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == 'l2':\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "nnbaXq_HXUpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load dataset from the hub\n",
        "dataset = load_dataset(\"fashion_mnist\")\n",
        "image_size = 28\n",
        "channels = 1\n",
        "batch_size = 128\n"
      ],
      "metadata": {
        "id": "YplR97XaXZOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# define image transformations (e.g. using torchvision)\n",
        "transform = Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "# define function\n",
        "def transforms(examples):\n",
        "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
        "   del examples[\"image\"]\n",
        "\n",
        "   return examples\n",
        "\n",
        "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
        "\n",
        "# create dataloader\n",
        "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "rxygOZMxXjeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataloader))\n",
        "print(batch.keys())\n"
      ],
      "metadata": {
        "id": "QGNNAt10XlpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_index):\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "    \n",
        "    # Equation 11 in the paper\n",
        "    # Use our model (noise predictor) to predict the mean\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        # Algorithm 2 line 4:\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
        "\n",
        "# Algorithm 2 (including returning all images)\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    b = shape[0]\n",
        "    # start from pure noise (for each example in the batch)\n",
        "    img = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "\n",
        "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
        "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
        "        imgs.append(img.cpu().numpy())\n",
        "    return imgs\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, image_size, batch_size=16, channels=3):\n",
        "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))\n"
      ],
      "metadata": {
        "id": "J3CPIOIkXo51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "save_and_sample_every = 1000\n"
      ],
      "metadata": {
        "id": "Dm_2vZwNXtMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Unet(\n",
        "    dim=image_size,\n",
        "    channels=channels,\n",
        "    dim_mults=(1, 2, 4,)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "qjXA1F9dXwg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "epochs = 6\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_size = batch[\"pixel_values\"].shape[0]\n",
        "      batch = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
        "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "      loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
        "\n",
        "      if step % 100 == 0:\n",
        "        print(\"Loss:\", loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # save generated images\n",
        "      if step != 0 and step % save_and_sample_every == 0:\n",
        "        milestone = step // save_and_sample_every\n",
        "        batches = num_to_groups(4, batch_size)\n",
        "        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
        "        all_images = torch.cat(all_images_list, dim=0)\n",
        "        all_images = (all_images + 1) * 0.5\n",
        "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)\n"
      ],
      "metadata": {
        "id": "sblW-xjHXzux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample 64 images\n",
        "# samples = sample(model, image_size=image_size, batch_size=64, channels=channels)\n",
        "\n",
        "# show a random one\n",
        "random_index = 5\n",
        "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "6wIoNfxK7Ql4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(samples[-1][7].reshape(image_size, image_size), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "TT7U2BBk75Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.animation as animation\n",
        "\n",
        "random_index = 53\n",
        "\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for i in range(timesteps):\n",
        "    plt.imshow(samples[i][random_index].reshape(image_size, image_size), cmap=\"gray\", animated=True)\n"
      ],
      "metadata": {
        "id": "N_NdEihk8YMZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}